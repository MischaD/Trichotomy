{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.path.dirname(os.path.abspath(\".\"))\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from einops import repeat\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def load_latents(basepath, model_name, n_splits=1):\n",
    "    \"\"\"load .pt latent for <model_name>. Assumes only n_splits .pt for each model\"\"\"\n",
    "    path = os.path.join(basepath, model_name)\n",
    "    files = os.listdir(path)\n",
    "    pts = [x for x in files if x.endswith(\".pt\") ]\n",
    "    csv = [x for x in files if x.endswith(\".csv\")]\n",
    "    assert len(pts) == n_splits and len(csv) == n_splits, f\"Unexpected number of csv/pts found in {path}\"\n",
    "\n",
    "    feature_path = os.path.join(path, pts[0])\n",
    "    print(f\"Loading features in {feature_path}\")\n",
    "    features = torch.load(feature_path)\n",
    "    paths = pd.read_csv(os.path.join(path, csv[0]))\n",
    "    return features, paths\n",
    "\n",
    "\n",
    "def update_matplotlib_font(fontsize=11, fontsize_ticks=8, tex=True, scale=1):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fontsize = scale * fontsize\n",
    "    fontsize_ticks = scale * fontsize_ticks\n",
    "    tex_fonts = {\n",
    "        # Use LaTeX to write all text\n",
    "        \"text.usetex\": tex,\n",
    "        \"font.family\": \"serif\",\n",
    "        # Use 11pt font in plots, to match 11pt font in document\n",
    "        \"axes.labelsize\": fontsize,\n",
    "        \"font.size\": fontsize,\n",
    "        # Make the legend/label fonts a little smaller\n",
    "        \"legend.fontsize\": fontsize_ticks,\n",
    "        \"xtick.labelsize\": fontsize_ticks,\n",
    "        \"ytick.labelsize\": fontsize_ticks\n",
    "    }\n",
    "    plt.rcParams.update(tex_fonts)\n",
    "\n",
    "ds_to_viz = {\n",
    "    \"mimic\": \"MIMIC-CXR\",\n",
    "    \"chexpert\": \"CheXpert\",\n",
    "    \"cxr8\": \"ChestX-ray8\"\n",
    "}\n",
    "\n",
    "\n",
    "update_matplotlib_font()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Add the directory containing edm2/generate.py to the Python path\n",
    "script_dir = Path(\"/vol/ideadata/ed52egek/pycharm/trichotomy/edm2\").resolve()  # Replace with the actual path\n",
    "sys.path.append(str(script_dir))\n",
    "\n",
    "# Now you can import functions from generate.py\n",
    "from generate_images import edm_sampler, StackedRandomGenerator\n",
    "from training.dataset import LatentDataset\n",
    "\n",
    "import pickle\n",
    "import dnnlib\n",
    "import torch\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import sys\n",
    "from torchvision.transforms import functional as F\n",
    "import torch\n",
    "\n",
    "class ToTensorIfNotTensor:\n",
    "    def __call__(self, input):\n",
    "        if isinstance(input, torch.Tensor):\n",
    "            return input\n",
    "        return F.to_tensor(input)\n",
    "\n",
    "\n",
    "def get_classification_model(model_path): \n",
    "    global class_labels\n",
    "    import os\n",
    "    import numpy as np\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.backends.cudnn as cudnn\n",
    "\n",
    "    import torchvision.transforms as T \n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import DataLoader\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    import torchvision\n",
    "\n",
    "    class DenseNet121(nn.Module):\n",
    "\n",
    "        def __init__(self, classCount, isTrained):\n",
    "        \n",
    "            super(DenseNet121, self).__init__()\n",
    "            \n",
    "            self.densenet121 = torchvision.models.densenet121(pretrained=isTrained)\n",
    "\n",
    "            kernelCount = self.densenet121.classifier.in_features\n",
    "            \n",
    "            self.densenet121.classifier = nn.Sequential(nn.Linear(kernelCount, classCount), nn.Sigmoid())\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.densenet121(x)\n",
    "            return x\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    #-------------------- SETTINGS: NETWORK ARCHITECTURE, MODEL LOAD\n",
    "    model = DenseNet121(len(class_labels), True).cuda()\n",
    "    model = model.cuda() \n",
    "\n",
    "    modelCheckpoint = torch.load(model_path)\n",
    "    state_dict = {k[7:]:v for k, v in modelCheckpoint['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "    class Classifier(nn.Module): \n",
    "        def __init__(self, model, transforms=\"default\") -> None:\n",
    "            super().__init__()\n",
    "            if transforms == \"default\": \n",
    "                normalize = T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "                transformList = []\n",
    "                #transformList.append(T.Resize(256)) -- forward pass during inference uses tencrop \n",
    "                transformList.append(T.Resize(226))\n",
    "                transformList.append(T.CenterCrop(226))\n",
    "                transformList.append(ToTensorIfNotTensor())\n",
    "                transformList.append(normalize)\n",
    "                self.transforms=T.Compose(transformList)\n",
    "            else: \n",
    "                self.transforms = transforms\n",
    "            self.model = model\n",
    "\n",
    "        def forward(self, x): \n",
    "            x_in = self.transforms(x)\n",
    "            return self.model(x_in)\n",
    "        \n",
    "        def lazy_foward(self, x): \n",
    "            # accepts tensor, 0-1, bchw \n",
    "            self.model.eval()\n",
    "            self.model.to(\"cuda\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                x_in = self.transforms(x)\n",
    "                if x_in.dim() == 3: \n",
    "                    x_in = x_in.unsqueeze(dim=0)\n",
    "                \n",
    "                varInput = x_in.cuda()\n",
    "\n",
    "                features = self.model.densenet121.features(varInput)\n",
    "                out = F.relu(features, inplace=True)\n",
    "                out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "                hidden_features = torch.flatten(out, 1)\n",
    "                out = self.model.densenet121.classifier(hidden_features)\n",
    "                #outMean = out.view(bs, ).mean(1)\n",
    "            return out.data, hidden_features.data\n",
    "\n",
    "    return Classifier(model)\n",
    "\n",
    "\n",
    "def get_privacy_model(path=\"/vol/ideadata/ed52egek/pycharm/trichotomy/privacy/archive/Siamese_ResNet50_allcxr/Siamese_ResNet50_allcxr_checkpoint.pth\"): \n",
    "    import torch\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Add the directory containing edm2/generate.py to the Python path\n",
    "    script_dir = Path(\"/vol/ideadata/ed52egek/pycharm/trichotomy/privacy\").resolve()  # Replace with the actual path\n",
    "    sys.path.append(str(script_dir))\n",
    "\n",
    "    from networks.SiameseNetwork import SiameseNetwork\n",
    "\n",
    "    net = SiameseNetwork()\n",
    "    net.load_state_dict(torch.load(path)[\"state_dict\"])\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def get_image_generation_model(path_net, path_gnet, model_weights, gmodel_weights, name, device=None): \n",
    "    if device is None: \n",
    "        device = \"cuda\"\n",
    "\n",
    "    encoder_batch_size = 4\n",
    "    max_batch_size = 32\n",
    "        # Rank 0 goes first.\n",
    "    net = path_net\n",
    "    gnet = path_gnet\n",
    "\n",
    "    # Load main network.\n",
    "    if isinstance(net, str):\n",
    "        print(f'Loading network from {net} ...')\n",
    "        with dnnlib.util.open_url(net, verbose=True) as f:\n",
    "            data = pickle.load(f)\n",
    "        net = data['ema'].to(device)\n",
    "        net.load_state_dict(torch.load(model_weights)[\"net\"])\n",
    "        \n",
    "        encoder = data.get('encoder', None)\n",
    "        encoder_mode = encoder.init_kwargs.encoder_norm_mode\n",
    "        encoder = dnnlib.util.construct_class_by_name(class_name='training.encoders.StabilityVAEEncoder', vae_name=encoder.init_kwargs.vae_name, encoder_norm_mode=encoder_mode)\n",
    "        print(f\"Encoder was initilized with {encoder._init_kwargs}\")\n",
    "\n",
    "    assert net is not None\n",
    "\n",
    "    # Load guidance network.\n",
    "    if isinstance(gnet, str):\n",
    "        print(f'Loading guidance network from {gnet} ...')\n",
    "        with dnnlib.util.open_url(gnet, verbose=True) as f:\n",
    "            data = pickle.load(f)\n",
    "        gnet = data['ema'].to(device)\n",
    "        gnet.load_state_dict(torch.load(gmodel_weights)[\"net\"])\n",
    "\n",
    "    assert gnet is not None\n",
    "\n",
    "    # Initialize encoder.\n",
    "    assert encoder is not None\n",
    "    print(f'Setting up {type(encoder).__name__}...')\n",
    "    encoder.init(device)\n",
    "    if encoder_batch_size is not None and hasattr(encoder, 'batch_size'):\n",
    "        encoder.batch_size = encoder_batch_size\n",
    "\n",
    "    return net, gnet, encoder\n",
    "\n",
    "\n",
    "def get_ds_and_indices(filelist=\"\", basedir=\"\", cond_mode=\"\", class_idx=None, N=100, n_per_index=1, one_per_subject=True, **kwargs): \n",
    "    # given a class index, basedir and potential moultiple n\n",
    "\n",
    "    train_ds = LatentDataset(filelist_txt=filelist, basedir=basedir, cond_mode=cond_mode, load_to_memory=False)\n",
    "\n",
    "    if class_idx is None: \n",
    "        indices = []\n",
    "        i = 0\n",
    "        last_subject_id = -1\n",
    "        while len(indices)  < N* n_per_index:\n",
    "            subject_id = int(train_ds.file_list[i].split(\"/\")[-1].split(\"_\")[0])\n",
    "            if subject_id == last_subject_id and one_per_subject: \n",
    "                i+=1\n",
    "                continue\n",
    "            else: \n",
    "                last_subject_id = subject_id\n",
    "                indices.extend([i,]*n_per_index)\n",
    "\n",
    "    else: \n",
    "    #if n_per_index != 1 and cond_mode==\"pseudo_cond\": \n",
    "\n",
    "    #    print(\"Generating multiple images with the same class label using n_per_index is the same as just generating more images for the same class\")\n",
    "\n",
    "    #    indices = torch.cat([torch.tensor([n,]*n_per_index) for n in range(N)]) \n",
    "    #else: \n",
    "        indices = []\n",
    "        i = 0\n",
    "        last_subject_id = -1\n",
    "        while len(indices)  < N* n_per_index:\n",
    "            if train_ds.label_list[i] == class_idx: \n",
    "                subject_id = int(train_ds.file_list[i].split(\"/\")[-1].split(\"_\")[0])\n",
    "                if subject_id == last_subject_id and one_per_subject: \n",
    "                    i+=1\n",
    "                    continue\n",
    "                else: \n",
    "                    last_subject_id = subject_id\n",
    "                    indices.extend([i,]*n_per_index)\n",
    "            i+=1\n",
    "    #indices = torch.cat(indices)\n",
    "\n",
    "    return train_ds, indices \n",
    "\n",
    "\n",
    "class ImageIterable:\n",
    "    def __init__(self, \n",
    "                 train_ds, \n",
    "                 device, \n",
    "                 net, \n",
    "                 sampler_fn, \n",
    "                 gnet, \n",
    "                 encoder, \n",
    "                 outdir=None, \n",
    "                 verbose=False, \n",
    "                 sampler_kwargs={},\n",
    "                 indices=[],\n",
    "                 max_batch_size=32, \n",
    "                 add_seed_to_path=True):\n",
    "        self.train_ds = train_ds\n",
    "        self.device = device\n",
    "        self.net = net\n",
    "        self.sampler_fn = sampler_fn\n",
    "        self.gnet = gnet\n",
    "        self.encoder = encoder\n",
    "        self.outdir = outdir\n",
    "        self.verbose = verbose\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.sampler_kwargs = sampler_kwargs\n",
    "\n",
    "        # Prepare seeds and batches\n",
    "        self.num_batches = max((len(indices) - 1) // max_batch_size + 1, 1)\n",
    "        self.rank_batches = np.array_split( np.arange(len(indices)), self.num_batches)\n",
    "        self.indices = np.array_split(np.array(indices), self.num_batches)\n",
    "        self.add_seed_to_path = add_seed_to_path\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Generating {len(self.seeds)} images...')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rank_batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch_idx in range(len(self.rank_batches)):\n",
    "            indices = self.indices[batch_idx]\n",
    "            r = dnnlib.EasyDict(images=None, labels=None, noise=None, \n",
    "                                batch_idx=batch_idx, num_batches=len(self.rank_batches), \n",
    "                                indices=indices, paths=None)\n",
    "            r.seeds =  self.rank_batches[batch_idx] \n",
    "            if len(r.seeds) > 0:\n",
    "                # Generate noise and labels\n",
    "                rnd = StackedRandomGenerator(self.device, r.seeds)\n",
    "                r.noise = rnd.randn([len(r.seeds), self.net.img_channels, self.net.img_resolution, self.net.img_resolution], device=self.device)\n",
    "                r.labels = torch.stack([self.train_ds.get_label(x) for x in r.indices]).to(self.device)\n",
    "                r.paths = [self.train_ds.file_list[x] for x in r.indices]\n",
    "\n",
    "                # Generate images\n",
    "                latents = dnnlib.util.call_func_by_name(func_name=self.sampler_fn, net=self.net, noise=r.noise,\n",
    "                                                        labels=r.labels, gnet=self.gnet, randn_like=rnd.randn_like, **self.sampler_kwargs)\n",
    "                r.images = self.encoder.decode(latents)\n",
    "\n",
    "                # Save images\n",
    "                if self.outdir is not None:\n",
    "                    for path, image, seed in zip(r.paths, r.images.permute(0, 2, 3, 1).cpu().numpy(), r.seeds):\n",
    "                        file_name = \"\".join(path.split(\".\")[:-1]) \n",
    "                        if self.add_seed_to_path: \n",
    "                            file_name += f\"_seed_{seed}.png\"\n",
    "                        else: \n",
    "                            file_name += \".png\"\n",
    "                        image_pth = os.path.join(self.outdir, file_name)\n",
    "\n",
    "                        os.makedirs(os.path.dirname(image_pth), exist_ok=True)\n",
    "                        PIL.Image.fromarray(image, 'RGB').save(image_pth)\n",
    "\n",
    "            # Yield results\n",
    "            yield r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class ImageDatasetReal(Dataset):\n",
    "    def __init__(self, root_dir, real_files, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing the image folders.\n",
    "            transform (callable, optional): Transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_list = real_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_info = self.image_list[idx]\n",
    "\n",
    "        full_path = os.path.join(self.root_dir, image_info['full_path'])\n",
    "        image = Image.open(full_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return {\n",
    "            'image': image,\n",
    "            'is_real': True,\n",
    "            'model_name': image_info['model_name'],\n",
    "            'class_name': image_info['class_name'],\n",
    "            'full_path': full_path,\n",
    "            'real_image_name': image_info['full_path']\n",
    "        }\n",
    "\n",
    "\n",
    "class SnthImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing the image folders.\n",
    "            transform (callable, optional): Transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_list = []\n",
    "        self._load_images()\n",
    "\n",
    "    def _load_images(self):\n",
    "        # Traverse the directory and collect image information\n",
    "        for model_name in os.listdir(self.root_dir):\n",
    "            model_path = os.path.join(self.root_dir, model_name)\n",
    "            if os.path.isdir(model_path):\n",
    "                for class_name in os.listdir(model_path):\n",
    "                    class_path = os.path.join(model_path, class_name)\n",
    "                    if os.path.isdir(class_path):\n",
    "                        images_path = os.path.join(class_path, 'images')\n",
    "                        if os.path.isdir(images_path):\n",
    "                            for image_name in os.listdir(images_path):\n",
    "                                if image_name.endswith('.png'):\n",
    "                                    # Extract real image name\n",
    "                                    real_image_name = '_'.join(image_name.split('_')[:2]) + '.png'\n",
    "                                    self.image_list.append({\n",
    "                                        'model_name': model_name,\n",
    "                                        'class_name': class_name,\n",
    "                                        'real_image_name': real_image_name,\n",
    "                                        'full_path': os.path.join(images_path, image_name)\n",
    "                                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_info = self.image_list[idx]\n",
    "        image = Image.open(image_info['full_path']).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return {\n",
    "            'image': image,\n",
    "            'is_real': False,\n",
    "            'model_name': image_info['model_name'],\n",
    "            'class_name': image_info['class_name'].replace(\"_\", \" \"), # No_finding --> No Finding \n",
    "            'full_path': image_info['full_path'],\n",
    "            'real_image_name': image_info['real_image_name']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "DEFAULT_CLF_PATH =  \"/vol/ideadata/ed52egek/pycharm/trichotomy/importantmodels/results_chexnet_real/saved_models_cxr8/m-05122024-131940.pth.tar\"\n",
    "DEFAULT_PRIV_PATH = \"/vol/ideadata/ed52egek/pycharm/trichotomy/privacy/archive/Siamese_ResNet50_allcxr/Siamese_ResNet50_allcxr_checkpoint.pth\"\n",
    "\n",
    "\n",
    "class DiADMSampleEvaluator(): \n",
    "    def __init__(self, device, clf_path=DEFAULT_CLF_PATH, priv_path=DEFAULT_PRIV_PATH) -> None:\n",
    "        self.privnet = get_privacy_model(path=priv_path) if priv_path is not None else get_privacy_model()\n",
    "        self.privnet = self.privnet.to(device)\n",
    "\n",
    "        self.clf_model = get_classification_model(clf_path)\n",
    "        self.clf_model = self.clf_model.to(device)\n",
    "\n",
    "    def predict(self, batch): \n",
    "        # 0 - 1, size does not matter\n",
    "        # batch[0] is real image, \n",
    "        # batch[1:] are synthetic images\n",
    "\n",
    "        pred, f_clf = self.clf_model.lazy_foward(batch)\n",
    "        clf_pred_scores = binary_cross_entropy(repeat(pred[0], \"f -> b f\", b=len(pred[1:])), pred[1:], reduction='none')\n",
    "        clf_pred_scores = clf_pred_scores.mean(dim=1)\n",
    "\n",
    "        real = repeat(batch[0], \"c h w -> b c h w\", b=len(batch[1:]))\n",
    "        snth = batch[1:]\n",
    "\n",
    "        priv_pred = self.privnet.lazy_pred(real, snth)\n",
    "        return clf_pred_scores, priv_pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"No Finding\", \"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Pleural Effusion\", \"Pneumonia\", \"Pneumothorax\"] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trichotomy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
